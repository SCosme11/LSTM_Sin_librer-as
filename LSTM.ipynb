{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66ef637",
   "metadata": {},
   "source": [
    "# LSTM para lenguaje natural a nivel de caracteres "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b4d7d",
   "metadata": {},
   "source": [
    "### Objetivo: <br>\n",
    "- Implementar una red neuronal de tipo LSTM (Long Short Term Memory) sin el uso de librerías especializadas. La única librería que uso es numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bc0ac",
   "metadata": {},
   "source": [
    "![Alt text](img/LSTM_rnn.png \"Title text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c037267",
   "metadata": {},
   "source": [
    "- La imagen muestra la estructura de una red neuronal recurrente que utiliza unidades LSTM.\n",
    "- Se puede observar que se repite la misma celda en diferentes pasos de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842772fb",
   "metadata": {},
   "source": [
    "![Alt text](img/LSTM.png \"Title text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec04032",
   "metadata": {},
   "source": [
    "* *t*: paso de tiempo.\n",
    "* *c*: Memoria a largo plazo.\n",
    "* *a*: Memoria a corto plazo. \n",
    "* Compuertas:\n",
    "    1. Compuerta de olvido: Decide qué información de la memoria de largo plazo ($c^{\\langle t-1 \\rangle}$) es irrelevante y debe borrarse.<br>Toma cómo entradas la memoria anterior a corto plazo ($a^{\\langle t-1 \\rangle}$) y el dato actual ($x^{\\langle t \\rangle}$)\n",
    "    2. Compuerta de Actualización: Decide que nueva información se va a guardar en la memoria de largo plazo.<br>\n",
    "    Operaciones:\n",
    "        * Se multiplican los vectores de la compuerta de actualización y de $\\tilde{c}^{\\langle t \\rangle}$ y despúes sumamos el resultado a $c^{\\langle t \\rangle}$.\n",
    "    3. Compuerta de Salida: Decide qué va a salir de la celda en este momento (el nuevo $a^{\\langle t \\rangle}$)\n",
    "    Operaciones:\n",
    "        * Toma la memoria de largo plazo ($c$) y la pasa por una tanh para normalizarla (-1 a 1).\n",
    "        * Filtra esa memoria usando una sigmoide (compuerta de salida).\n",
    "        * El resultado es el nuevo estado oculto $a^{\\langle t \\rangle}$.\n",
    "* Predicción ($y^{\\langle t \\rangle}$): El estado oculto $a^{\\langle t \\rangle}$ entra a una capa densa con activación Softmax. En este caso es un vector de probabilidad de 52 dimensiones (caracteres válidos) que dice cuál es la letra más probable que sigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5cb5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cab92",
   "metadata": {},
   "source": [
    "* Función de activación para todas las compuertas.\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "004f1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfc427",
   "metadata": {},
   "source": [
    "- Función que utilizamos para la predicción: $$ \\text{softmax}(x) = \\frac{e^{x - \\max(x)}}{\\sum e^{x - \\max(x)}} $$ donde *x* es $a^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5253ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) #Restamos el máximo para estabilidad numérica.\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f80bdf",
   "metadata": {},
   "source": [
    "## Forward Propagation (cómo hacemos predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0e11b",
   "metadata": {},
   "source": [
    "- Trabajamos con muchos pesos y para que nuestra red neuronal aprenda es necesario inicializar de manera aleatoria los pesos. \n",
    "- Los guardamos en las siguientes matrices: (**n_a**: unidades (neuronas) ocultas (100), **n_x**: unidades de entrada (52), **n_y**: unidades de salida (52))\n",
    "    * *$W_{f}$*: Matriz con los pesos de la compuerta de olvido. *Dim*: (n_a, n_a + n_x) = (100, 152).\n",
    "    * *$b_{f}$*: Vector con los bias de la compuerta de olvido. *Dim*: (n_a, 1) = (100, 1).\n",
    "    * *$W_{i}$*: Matriz con los pesos de la compuerta de actualización. *Dim*: (n_a, n_a + n_x) = (100, 152).\n",
    "    * *$b_{i}$*: Vector con los bias de la compuerta de actualización. *Dim*: (n_a, 1) = (100, 1).\n",
    "    * *$W_{c}$*: Matriz con los pesos de la compuerta candidata ($\\tilde{c}^{\\langle t \\rangle}$). *Dim*: (n_a, n_a + n_x) = (100, 152).\n",
    "    * *$b_{c}$*: Vector con los bias de la compuerta candidata ($\\tilde{c}^{\\langle t \\rangle}$). *Dim*: (n_a, 1) = (100, 1)\n",
    "    * *$W_{o}$*: Matriz con los pesos de la compuerta de salida. *Dim*: (n_a, n_a + n_x) = (100, 152).\n",
    "    * *$b_{o}$*: Vector con los bias de la compuerta de salida. *Dim*: (n_a, 1) = (100, 1)\n",
    "    * *$W_{y}$*: Matriz con los pesos para la predicción. *Dim*: (n_y, n_a) = (52, 100).\n",
    "    * *$b_{y}$*: Vector con los bias para la predicción. *Dim*: (n_y, 1) = (52, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8357b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Inicializa los parámetros para la LSTM\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    \n",
    "    # Xavier initialization (escala los pesos para que no sean muy grandes)\n",
    "    std = np.sqrt(2.0 / (n_a + n_x))\n",
    "    \n",
    "    # Pesos y sesgos para las puertas (Forget, Update, Output, Candidate)\n",
    "    parameters['Wf'] = np.random.randn(n_a, n_a + n_x) * std\n",
    "    parameters['bf'] = np.zeros((n_a, 1))\n",
    "    parameters['Wi'] = np.random.randn(n_a, n_a + n_x) * std\n",
    "    parameters['bi'] = np.zeros((n_a, 1))\n",
    "    parameters['Wc'] = np.random.randn(n_a, n_a + n_x) * std\n",
    "    parameters['bc'] = np.zeros((n_a, 1))\n",
    "    parameters['Wo'] = np.random.randn(n_a, n_a + n_x) * std\n",
    "    parameters['bo'] = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Pesos para la predicción (Output layer)\n",
    "    parameters['Wy'] = np.random.randn(n_y, n_a) * std\n",
    "    parameters['by'] = np.zeros((n_y, 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad6b6c",
   "metadata": {},
   "source": [
    "* En cada celda de nuestra red neuronal suceden las siguientes operaciones.\n",
    "1. Concatenamos $a^{\\langle t-1 \\rangle}$ con $x^{\\langle t \\rangle}$.\n",
    "2. Calculamos las compuertas.\n",
    "    - Compuerta de olvido: $$ \\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f \\cdot [a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f) $$\n",
    "    - Compuerta de actualización: $$ \\Gamma_u^{\\langle t \\rangle} = \\sigma(W_i \\cdot [a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_i) $$\n",
    "    - Compuerta candidata: $$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c \\cdot [a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c) $$\n",
    "        - Actualizamos $c^{\\langle t \\rangle}$ usando $\\tilde{c}^{\\langle t \\rangle}$, $\\Gamma_u^{\\langle t \\rangle}$ y $\\Gamma_f^{\\langle t \\rangle}$: <br>$$c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}\\cdot c^{\\langle t-1 \\rangle}+\\Gamma_u^{\\langle t \\rangle}\\cdot \\tilde{c}^{\\langle t \\rangle}$$\n",
    "- Compuerta Oculta: $$\\Gamma_o^{\\langle t \\rangle}=\\sigma(W_o \\cdot [a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)$$\n",
    "- Activación $$a^{\\langle t \\rangle}=\\Gamma_o^{\\langle t \\rangle}\\cdot \\tanh(c^{\\langle t \\rangle})$$\n",
    "- **Predicción:** $$y^{\\langle t \\rangle}=\\text{softmax}(W_y \\cdot a^{\\langle t \\rangle}+b_y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cca5d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Un solo paso de tiempo del LSTM.\n",
    "    \"\"\"\n",
    "    # Recuperar parámetros\n",
    "    Wf = parameters[\"Wf\"]; bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]; bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]; bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]; bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]; by = parameters[\"by\"]\n",
    "    \n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # 1. Concatenar a_prev y xt\n",
    "    concat = np.concatenate((a_prev, xt), axis=0)\n",
    "\n",
    "    # 2. Calcular compuertas\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)        # Forget gate\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)        # Update gate\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)       # Candidate value\n",
    "    c_next = ft * c_prev + it * cct              # Cell state update\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)        # Output gate\n",
    "    a_next = ot * np.tanh(c_next)                # Hidden state output\n",
    "    \n",
    "\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    # Guardar caché para backprop\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f956a0",
   "metadata": {},
   "source": [
    "* La celda anterior es para calcular un paso de tiempo dentro de la red neuronal, la siguiente función ejecuta el LSTM sobre todos los pasos de tiempo ($T_x$).\n",
    "* Para nuestra arquitectura, utilizaremos $T_x=25$, lo que significa que la red neuronal mira una ventana de 25 caracteres pasados para intentar predecir el siguiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecaa21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Ejecuta el LSTM sobre todos los pasos de tiempo T_x.\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    n_x, m, T_x = x.shape #(52, 1, 25) = (número de caracteres, batch size, time steps).\n",
    "    n_y, n_a = parameters['Wy'].shape #(52, 100)\n",
    "    \n",
    "    # Inicializar tensores de salida\n",
    "    a = np.zeros((n_a, m, T_x)) #(100, 1, 25)\n",
    "    c = np.zeros((n_a, m, T_x)) #(100, 1, 25)\n",
    "    y = np.zeros((n_y, m, T_x)) #(52, 1, 25)\n",
    "    \n",
    "    a_next = a0 \n",
    "    c_next = np.zeros((n_a, m)) #(100, 1)\n",
    "    \n",
    "    # Loop sobre el tiempo\n",
    "    for t in range(T_x):\n",
    "        xt = x[:,:,t]\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "        \n",
    "        a[:,:,t] = a_next\n",
    "        c[:,:,t] = c_next\n",
    "        y[:,:,t] = yt\n",
    "        caches.append(cache)\n",
    "        \n",
    "    caches = (caches, x)\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14534c16",
   "metadata": {},
   "source": [
    "## Backpropagation (cómo ajustamos los pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e6f13",
   "metadata": {},
   "source": [
    "#### Para actualizar los pesos usamos el descenso del gradiente, esto nos sirve para minimizar la función de perdida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef0997",
   "metadata": {},
   "source": [
    "* La función de perdida que utilizamos es la entropía cruzada categórica dado que es un problema de clasificación multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d352f",
   "metadata": {},
   "source": [
    "$$ L = - \\log(\\hat{y}_k) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b494636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_hat, y_indices):\n",
    "    \"\"\"\n",
    "    Calcula la pérdida Cross-Entropy\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    # y_hat tiene shape (n_y, m, T_x). Aquí m=1\n",
    "    for t in range(len(y_indices)):\n",
    "        # Probabilidad que la red asignó al carácter correcto\n",
    "        prob = y_hat[y_indices[t], 0, t] \n",
    "        loss -= np.log(prob)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e0eb8",
   "metadata": {},
   "source": [
    "#### Cálculo de los gradientes para un solo time step:\n",
    "Lo que queremos encontrar es lo siguiente: $$\\frac{\\partial L}{\\partial W_f}, \\frac{\\partial L}{\\partial W_i}, \\frac{\\partial L}{\\partial W_c}, \\frac{\\partial L}{\\partial W_o}$$ $$\\frac{\\partial L}{\\partial b_f}, \\frac{\\partial L}{\\partial b_i}, \\frac{\\partial L}{\\partial b_c}, \\frac{\\partial L}{\\partial b_o}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09a7c3",
   "metadata": {},
   "source": [
    "#### Es necesario utilizar la regla de la cadena:\n",
    "$$\\frac{\\partial L}{\\partial W_f}=\\frac{\\partial L}{\\partial c^{\\langle t \\rangle}} \\frac{\\partial c^{\\langle t \\rangle}}{\\partial \\Gamma_f^{\\langle t \\rangle}} \\frac{\\partial \\Gamma_f^{\\langle t \\rangle}}{\\partial z_f^{\\langle t \\rangle}} \\frac{\\partial z_f^{\\langle t \\rangle}}{\\partial W_f}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd806f9",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial W_i} = \\frac{\\partial L}{\\partial c^{\\langle t \\rangle}} \\frac{\\partial c^{\\langle t \\rangle}}{\\partial \\Gamma_i^{\\langle t \\rangle}} \\frac{\\partial \\Gamma_i^{\\langle t \\rangle}}{\\partial z_i^{\\langle t \\rangle}} \\frac{\\partial z_i^{\\langle t \\rangle}}{\\partial W_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d4f91",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial W_c} = \\frac{\\partial L}{\\partial c^{\\langle t \\rangle}} \\frac{\\partial c^{\\langle t \\rangle}}{\\partial \\tilde{c}^{\\langle t \\rangle}} \\frac{\\partial \\tilde{c}^{\\langle t \\rangle}}{\\partial z_c^{\\langle t \\rangle}} \\frac{\\partial z_c^{\\langle t \\rangle}}{\\partial W_c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7511a4",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial W_o} = \\frac{\\partial L}{\\partial a^{\\langle t \\rangle}} \\frac{\\partial a^{\\langle t \\rangle}}{\\partial \\Gamma_o^{\\langle t \\rangle}} \\frac{\\partial \\Gamma_o^{\\langle t \\rangle}}{\\partial z_o^{\\langle t \\rangle}} \\frac{\\partial z_o^{\\langle t \\rangle}}{\\partial W_o}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "388c1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Calcula gradientes para un solo paso de tiempo.\n",
    "    \"\"\"\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    # Derivadas de las compuertas \n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (dc_next * it + ot * (1 - np.tanh(c_next)**2) * it * da_next) * (1 - cct**2)\n",
    "    dit = (dc_next * cct + ot * (1 - np.tanh(c_next)**2) * cct * da_next) * it * (1 - it)\n",
    "    dft = (dc_next * c_prev + ot * (1 - np.tanh(c_next)**2) * c_prev * da_next) * ft * (1 - ft)\n",
    "\n",
    "    # Gradientes de los parámetros\n",
    "    concat = np.concatenate((a_prev, xt), axis=0)\n",
    "    dWf = np.dot(dft, concat.T)\n",
    "    dWi = np.dot(dit, concat.T)\n",
    "    dWc = np.dot(dcct, concat.T)\n",
    "    dWo = np.dot(dot, concat.T)\n",
    "    \n",
    "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
    "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
    "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
    "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
    "\n",
    "    # Gradientes para pasar al paso anterior (prev)\n",
    "    # Recuperamos pesos Wf, Wi, etc.\n",
    "    Wf, Wi, Wc, Wo = parameters['Wf'], parameters['Wi'], parameters['Wc'], parameters['Wo']\n",
    "    \n",
    "    da_prev = (np.dot(Wf[:, :n_a].T, dft) + np.dot(Wi[:, :n_a].T, dit) + \n",
    "               np.dot(Wc[:, :n_a].T, dcct) + np.dot(Wo[:, :n_a].T, dot))\n",
    "               \n",
    "    dc_prev = dc_next * ft + ot * (1 - np.tanh(c_next)**2) * ft * da_next\n",
    "    \n",
    "    dxt = (np.dot(Wf[:, n_a:].T, dft) + np.dot(Wi[:, n_a:].T, dit) + \n",
    "           np.dot(Wc[:, n_a:].T, dcct) + np.dot(Wo[:, n_a:].T, dot))\n",
    "    \n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \n",
    "                 \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
    "                 \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36b9ee",
   "metadata": {},
   "source": [
    "* Dado que estamos trabajando con una red neuronal recurrente, es necesario calcular los gradientes a través del tiempo. Mientras que lstm_cell_backward calcula las derivadas de un solo instante, lstm_backward actúa como el coordinador temporal que recorre la secuencia desde el final hacia el principio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76cc79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Backprop a través del tiempo.\n",
    "    \"\"\"\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # Inicializar gradientes acumulados\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    dWf = np.zeros((n_a, n_a + n_x)); dWi = np.zeros_like(dWf)\n",
    "    dWc = np.zeros_like(dWf); dWo = np.zeros_like(dWf)\n",
    "    dbf = np.zeros((n_a, 1)); dbi = np.zeros_like(dbf)\n",
    "    dbc = np.zeros_like(dbf); dbo = np.zeros_like(dbf)\n",
    "    \n",
    "    # Loop hacia atrás\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Gradiente total en t = Gradiente externo (da) + Gradiente del futuro (da_prevt)\n",
    "        gradients = lstm_cell_backward(da[:,:,t] + da_prevt, dc_prevt, caches[t])\n",
    "        \n",
    "        da_prevt = gradients[\"da_prev\"]\n",
    "        dc_prevt = gradients[\"dc_prev\"]\n",
    "        dx[:,:,t] = gradients[\"dxt\"]\n",
    "        \n",
    "        # Acumular gradientes de pesos\n",
    "        dWf += gradients[\"dWf\"]; dWi += gradients[\"dWi\"]\n",
    "        dWc += gradients[\"dWc\"]; dWo += gradients[\"dWo\"]\n",
    "        dbf += gradients[\"dbf\"]; dbi += gradients[\"dbi\"]\n",
    "        dbc += gradients[\"dbc\"]; dbo += gradients[\"dbo\"]\n",
    "        \n",
    "    da0 = da_prevt\n",
    "\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf, \"dbf\": dbf, \"dWi\": dWi, \"dbi\": dbi,\n",
    "                \"dWc\": dWc, \"dbc\": dbc, \"dWo\": dWo, \"dbo\": dbo}\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d401f0",
   "metadata": {},
   "source": [
    "* Vamos a utilizar un optimizador para que el proceso del descenso del gradiente sea más eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc224f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \"\"\"\n",
    "    Inicializa v y s como diccionarios con ceros.\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # número de capas/tipos de parámetros\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        v[key] = np.zeros(parameters[key].shape)\n",
    "        s[key] = np.zeros(parameters[key].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80121b53",
   "metadata": {},
   "source": [
    "* Actualizamos los pesos (parámetros) utilizando Adam de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a39f53",
   "metadata": {},
   "source": [
    "1. Cálculo del momento: media exponencial de los gradientes pasados:\n",
    "$$v_t = \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla J(\\theta_t)$$\n",
    "2. Corrección de sesgo de *v*:\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_1^t}$$\n",
    "3. Cálculo de s (RMSprop): media exponencial de los gradientes al cuadrado:\n",
    "$$s_t = \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(\\theta_t))^2$$\n",
    "4. Corrección de sesgo para *s*:\n",
    "$$\\hat{s}_t = \\frac{s_t}{1 - \\beta_2^t}$$\n",
    "5. **Actualización de parámetros:**\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{v}_t}{\\sqrt{\\hat{s}_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14aebcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Recorta los gradientes para evitar explosión (Exploding Gradients)\n",
    "    '''\n",
    "    for gradient in [gradients[key] for key in gradients.keys()]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    return gradients\n",
    "\n",
    "def update_parameters_with_adam(parameters, gradients, v, s, t, learning_rate=0.01, \n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Actualiza parámetros usando Adam.\n",
    "    \"\"\"\n",
    "    v_corrected = {}                         # Inicializar primera estimación de momento\n",
    "    s_corrected = {}                         # Inicializar segunda estimación de momento\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        # --- Cálculo de v (Momento) ---\n",
    "        v[key] = beta1 * v[key] + (1 - beta1) * gradients['d' + key]\n",
    "        # Corrección de sesgo para v\n",
    "        v_corrected[key] = v[key] / (1 - np.power(beta1, t))\n",
    "        \n",
    "        # --- Cálculo de s (RMSprop) ---\n",
    "        # Importante: gradients al cuadrado\n",
    "        s[key] = beta2 * s[key] + (1 - beta2) * np.power(gradients['d' + key], 2)\n",
    "        # Corrección de sesgo para s\n",
    "        s_corrected[key] = s[key] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        # --- Actualización de parámetros ---\n",
    "        parameters[key] -= learning_rate * (v_corrected[key] / (np.sqrt(s_corrected[key]) + epsilon))\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a09cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(parameters, v, s, file_name='checkpoint_kafka.npz'):\n",
    "    \"\"\"\n",
    "    Guarda parámetros Y el estado del optimizador (v, s) en un solo archivo.\n",
    "    Usa prefijos 'v_' y 's_' para distinguirlos.\n",
    "    \"\"\"\n",
    "    save_dict = {}\n",
    "    \n",
    "    # 1. Guardar parámetros normales\n",
    "    for key, value in parameters.items():\n",
    "        save_dict[key] = value\n",
    "        \n",
    "    # 2. Guardar v (con prefijo v_)\n",
    "    for key, value in v.items():\n",
    "        save_dict['v_' + key] = value\n",
    "        \n",
    "    # 3. Guardar s (con prefijo s_)\n",
    "    for key, value in s.items():\n",
    "        save_dict['s_' + key] = value\n",
    "        \n",
    "    np.savez(file_name, **save_dict)\n",
    "    print(f\"¡Checkpoint guardado! (Params + Adam states) en: {file_name}\")\n",
    "\n",
    "def load_checkpoint(file_name='checkpoint_kafka.npz'):\n",
    "    \"\"\"\n",
    "    Carga params, v y s separándolos por sus prefijos.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"No se encontró {file_name}. Se iniciará desde cero.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    loaded = np.load(file_name)\n",
    "    parameters = {}\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for key in loaded.files:\n",
    "        if key.startswith('v_'):\n",
    "            # Es parte de v (quitamos el prefijo 'v_')\n",
    "            original_key = key[2:]\n",
    "            v[original_key] = loaded[key]\n",
    "        elif key.startswith('s_'):\n",
    "            # Es parte de s (quitamos el prefijo 's_')\n",
    "            original_key = key[2:]\n",
    "            s[original_key] = loaded[key]\n",
    "        else:\n",
    "            # Es un parámetro normal\n",
    "            parameters[key] = loaded[key]\n",
    "            \n",
    "    print(\"¡Checkpoint cargado correctamente! El optimizador recuerda dónde se quedó.\")\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f605faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_k(parameters, char_to_ix, seed, temperature=1.0, k=5):\n",
    "    \"\"\"\n",
    "    Genera texto usando Top-K Sampling para evitar bucles.\n",
    "    k: Solo considera las k letras más probables en cada paso.\n",
    "    \"\"\"\n",
    "    Wf, bf = parameters['Wf'], parameters['bf']\n",
    "    Wi, bi = parameters['Wi'], parameters['bi']\n",
    "    Wc, bc = parameters['Wc'], parameters['bc']\n",
    "    Wo, bo = parameters['Wo'], parameters['bo']\n",
    "    Wy, by = parameters['Wy'], parameters['by']\n",
    "    \n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Wf.shape[0]\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    c_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Inicializar\n",
    "    if seed in char_to_ix:\n",
    "        idx = char_to_ix[seed]\n",
    "    else:\n",
    "        idx = char_to_ix[' '] # Espacio por defecto si la semilla falla\n",
    "        \n",
    "    x[idx] = 1\n",
    "    indices = []\n",
    "    \n",
    "    for _ in range(300): # Generar 300 caracteres\n",
    "        # Forward\n",
    "        a_next, c_next, y_hat, _ = lstm_cell_forward(x, a_prev, c_prev, parameters)\n",
    "        \n",
    "        # --- TOP-K LOGIC ---\n",
    "        p = y_hat.ravel()\n",
    "        \n",
    "        # Aplicar temperatura\n",
    "        p = np.log(p + 1e-10) / temperature\n",
    "        p = np.exp(p) / np.sum(np.exp(p))\n",
    "        \n",
    "        # Ordenar probabilidades y quedarse con las top K\n",
    "        top_k_indices = np.argsort(p)[-k:] # Índices de las k letras más probables\n",
    "        top_k_probs = p[top_k_indices]\n",
    "        \n",
    "        # Renormalizar para que sumen 1\n",
    "        top_k_probs = top_k_probs / np.sum(top_k_probs)\n",
    "        \n",
    "        # Elegir una de las top k\n",
    "        choice = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "        \n",
    "        indices.append(choice)\n",
    "        \n",
    "        # Preparar siguiente paso\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[choice] = 1\n",
    "        a_prev = a_next\n",
    "        c_prev = c_next\n",
    "        \n",
    "    return indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98836056",
   "metadata": {},
   "source": [
    "## Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489c110",
   "metadata": {},
   "source": [
    "1. El modelo revisa si retoma los pesos que ya se habían entrenado o si empieza de cero. <br>\n",
    "2. Entramos al bucle de entrenamiento: <br>\n",
    "    * Preparamos los datos y utilizamos una ventana deslizante para moverse por el data set. <br>La entrada serán 25 caracters y el objetivo son también 25 caracteres pero desplazados un lugar a la derecha ya que queremos que nuestra red prediga el siguiente caracter dado el actual.<br>\n",
    "    * One-hot encoding: convierte los caracteres a vectores de ceros y unos para que puedan ser procesados por la red neuronal.\n",
    "3. **Forward propagation:** inicializamos en ceros el estado oculto inicial, pasamos los vectores one-hot por la red y obtenemos la predicción.\n",
    "4. Calculamos la perdida utilizando un prmoedio móvil exponencial para suavizar (esto sirve sólo para la visulización).\n",
    "5. **Backward propagation:** calculamos los gradientes para ver como deben de cambiar los pesos. \n",
    "6. Actualización de parámetros: modifica los pesos de la red utilizando Adam.\n",
    "7. Cada 500 iteraciones imprimimos la pérdida, generamos texto y guardamos los pesos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "541aea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, char_to_ix, ix_to_char, num_iterations=3000, n_a=50, vocab_size=52, \n",
    "          learning_rate=0.005, checkpoint_file='checkpoint_kafka.npz'):\n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size #tamáño de los vocabularios (52 caracteres)\n",
    "    \n",
    "    #Cargamos los parámetros que ya teníamos de los entrenamientos pasados\n",
    "    parameters, v, s = load_checkpoint(checkpoint_file) \n",
    "    \n",
    "    # Si es la primera vez entrenando\n",
    "    if parameters is None:\n",
    "        print(\"Iniciando entrenamiento desde cero...\")\n",
    "        parameters = initialize_parameters(n_a, n_x, n_y) #Inicializamos parámetros aleatoriamente. \n",
    "        v, s = initialize_adam(parameters) #Inicializamos v y s cómo vectores con puros ceros. \n",
    "    else:\n",
    "        print(\"Reanudando entrenamiento con estado previo...\")\n",
    "        # Si el archivo existía pero por alguna razón v o s no (versiones viejas), los reinicializamos por seguridad.\n",
    "        if not v or not s:\n",
    "            print(\"Advertencia: Se encontraron pesos pero no estado del optimizador. Reiniciando Adam.\")\n",
    "            v, s = initialize_adam(parameters)\n",
    "\n",
    "    # Inicializar pérdida\n",
    "    loss = -np.log(1.0/vocab_size)*25 #probabilidad totalmente al azar dado que todos los caracteres \n",
    "    \n",
    "    # Bucle de entrenamiento\n",
    "    for j in range(num_iterations):\n",
    "        index = j % len(data)\n",
    "        if index + 25 + 1 > len(data): index = 0\n",
    "        single_example = data[index : index + 25]\n",
    "        single_target = data[index + 1 : index + 26]\n",
    "        X_indices = [char_to_ix[ch] for ch in single_example] #pasamos de caracter a entero\n",
    "        Y_indices = [char_to_ix[ch] for ch in single_target] #pasamos de caracter a entero\n",
    "        \n",
    "        x_one_hot = np.zeros((n_x, 1, 25))\n",
    "        for step, idx in enumerate(X_indices):\n",
    "            x_one_hot[idx, 0, step] = 1\n",
    "\n",
    "        # 1. Forward\n",
    "        a0 = np.zeros((n_a, 1)) #estado oculto inicial en ceros\n",
    "        a, y_hat, c, caches = lstm_forward(x_one_hot, a0, parameters) #pasa los datos por la red\n",
    "        \n",
    "        # 2. Loss \n",
    "        curr_loss = compute_loss(y_hat, Y_indices)\n",
    "        if j == 0: \n",
    "            loss = curr_loss\n",
    "        else: \n",
    "            loss = loss * 0.999 + curr_loss * 0.001\n",
    "        \n",
    "        # 3. Backward\n",
    "        dy = np.copy(y_hat)\n",
    "        for step, idx in enumerate(Y_indices):\n",
    "            dy[idx, 0, step] -= 1\n",
    "        gradients = lstm_backward(np.zeros_like(a), caches)\n",
    "        \n",
    "        # Gradientes capa salida\n",
    "        gradients['dWy'] = np.zeros_like(parameters['Wy'])\n",
    "        gradients['dby'] = np.zeros_like(parameters['by'])\n",
    "        da = np.zeros_like(a)\n",
    "        for step in reversed(range(len(X_indices))):\n",
    "            gradients['dWy'] += np.dot(dy[:,:,step], a[:,:,step].T)\n",
    "            gradients['dby'] += dy[:,:,step]\n",
    "            da[:,:,step] = np.dot(parameters['Wy'].T, dy[:,:,step])\n",
    "        grads_lstm = lstm_backward(da, caches)\n",
    "        gradients.update(grads_lstm)\n",
    "        \n",
    "        # 4. Update\n",
    "        gradients = clip(gradients, 5)\n",
    "        # Usamos t = j + 1, pero idealmente guardaríamos 't' también. \n",
    "        # Para simplificar, reiniciar t no es tan grave como reiniciar v/s.\n",
    "        parameters, v, s = update_parameters_with_adam(parameters, gradients, v, s, j+1, learning_rate)\n",
    "\n",
    "        # Log y Guardado\n",
    "        if j % 500 == 0:\n",
    "            print(f\"Iteración {j}, Pérdida: {loss:.4f}\")\n",
    "            try:\n",
    "                seed = data[index]\n",
    "                indices_generated = sample_top_k(parameters, char_to_ix, seed)\n",
    "                print(\"Generado:\", ''.join([ix_to_char[ix] for ix in indices_generated]).replace('\\n', ' '))\n",
    "            except:\n",
    "                pass\n",
    "            print('-'*50)\n",
    "            \n",
    "            # GUARDAR AQUÍ AUTOMÁTICAMENTE\n",
    "            save_checkpoint(parameters, v, s, checkpoint_file)\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94ca9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(parameters, file_name='checkpoint_kafka.npz'):\n",
    "    \"\"\"\n",
    "    Guarda el diccionario de parámetros en un archivo .npz\n",
    "    \"\"\"\n",
    "    # El operador ** desempaqueta el diccionario para guardarlo con sus nombres clave\n",
    "    np.savez(file_name, **parameters)\n",
    "    print(f\"¡Éxito! Parámetros guardados en: {file_name}\")\n",
    "    \n",
    "\n",
    "def load_parameters(file_name='checkpoint_kafka.npz'):\n",
    "    \"\"\"\n",
    "    Carga los pesos desde el archivo y reconstruye el diccionario.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"Error: No se encuentra el archivo {file_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Cargar el archivo\n",
    "    loaded = np.load(file_name)\n",
    "    \n",
    "    # Reconstruir el diccionario de Python (np.load devuelve un objeto especial)\n",
    "    parameters = {}\n",
    "    for key in loaded.files:\n",
    "        parameters[key] = loaded[key]\n",
    "        \n",
    "    print(\"¡Pesos cargados correctamente! Tu red ya sabe escribir.\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6e04061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Checkpoint cargado correctamente! El optimizador recuerda dónde se quedó.\n",
      "Reanudando entrenamiento con estado previo...\n",
      "Advertencia: Se encontraron pesos pero no estado del optimizador. Reiniciando Adam.\n",
      "Iteración 0, Pérdida: 68.0088\n",
      "Generado: asazrar de su enter ara nista y preferidal con caer en su esera quer prefería quera de ningún puese fueres en dera de ningún y genían ese podía doba podí el de no se dejada se heciento en esta perer colgando de esta ser en esta prefería quedentando de núera podía doberidar, perder logra modo es, pre\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 500, Pérdida: 55.6805\n",
      "Generado: o, precuas con el relantes perertido en coso de se desaba muchada. pensorde en en en punto sober, partión el sere teníación desalpulamente emen en para partes al sabe, patamententruido, con el cobírio ciónseño insoba, al teníarace de la cabeza sue sulgado se desbeve en compata, perorante con ezement\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 1000, Pérdida: 47.0201\n",
      "Generado: o un bonito habíatado doradosa doraar. reprede hacía en en resaco pocon fueta samba erococococorque de hado atado poco bostado domasa, y ere colocado endaba una ata con tavimdana dorerto habitar aala cama sabris paquetado dontido a a una davía habío habían la cuadro dobía de orecon una dama antecosq\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 1500, Pérdida: 42.3165\n",
      "Generado: cidase oíta muy melaame alhoy mhorio del se esponsa socolvedurida se entos dobsobre, los orse conseclave pesaría el muy mela su ante y masa sobsiltas su ardas entra sobre la chos sobre las chifla y olviéte lo que pero estomar de la mor despasas colocadodo porque había una después pesor estaba adí su\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 2000, Pérdida: 39.4031\n",
      "Generado:  en coma lado. si su empeño ciun heía dura lanía día encon del seleuver estan dura y otrtide fienguido posi nis hesen un dío y osto y otro del también las cotido un bíor de vien orería profesión sotdo un pero en el cotumbó. no itober de viaje. los ses doríor dolo, sentido, y otrtós en su em sí y ura\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 2500, Pérdida: 37.2601\n",
      "Generado: da porque para dese la ces de la cama una el la cabeza cerra se ser de la cabaza lo lo que ntraba en sen la contaba escamente  de la le le con qué el comente más cer se ve perque lentaba tró espaca al ca de la cabeza dores corde la contró sintre parte las cirje la encontró cama li cambiiata se lara \n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 3000, Pérdida: 36.2917\n",
      "Generado: ite los pedoros pequeños. qué levantartos pedidos hace  tampceltió al sobresal. lences. sentar espués pensión pedoro poderes vivel, a nuevdiaros ble, lados pensión pasar estosmplogon limentiónse que vivuanse vivel es humblalosar i levanto mo limeso sermente cesva deslizó tento pedosque de lonzasa se\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 3500, Pérdida: 35.2508\n",
      "Generado: estro hablar hacia esa, habla, por me hablar haciaacalar paduería la mesa dombridarme paducera, de toda de sertafe, de se padar el con levantarmenta ado hacia cera, saba del la manto ema y ados con el empltados mi que hace abuajanterma habllado dejora he demás, paro el empltabría hacia adorde duvía \n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 4000, Pérdida: 35.0294\n",
      "Generado: e levantadia sengró a las mencas y mieltador y maegunas seis sentado, y mó tan sos y media ojes tren sipalargo el pros algo cin tió miempadidada malgado cuantesan hacia cendabo padrise las seis y lormano y sin las ertar haco el padres tiencora las cinco, y machos y destar torados ticto. hae ción la \n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 4500, Pérdida: 33.9840\n",
      "Generado: sto. torao a hacer ahorar las siete, para corsiato que tanto. sí, huguiente tren segula a sabera desa para hacer ahas las siete, para tren salía sosalaba seguienguil quién las siatuido, sí, tenguión tren salía era algador. quilo, por siguieno tranquilo, tante a las sietente. qué ten hacer able caso \n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 5000, Pérdida: 33.4532\n",
      "Generado:  del segumente desagro deshagado que habría duisería sufrrmo porque si estos o.trros pasaría suescermo desloco y sonajas y seguiridos pesa que nis se ya entraba esto dería sormose descamo del jefesostrio despartampodos ys habría dado desto dedo descuchaba porque habría dube so habría dosio sugaberer\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 5500, Pérdida: 34.2519\n",
      "Generado: rtendría que ha extente sugumenter algado de su excentror desvaría de la sueño, en octro enconbuenier al médico des una sento después después de sueño, se encontraba orado de las objeciones ero a exte senco a lo sunten halvorio trerorpe de los resto, su excegor dombres cinco exterio. por esperco des\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 6000, Pérdida: 34.0529\n",
      "Generado:  tuelodos, son a sue,, en entedres le lalmenoe las ciuchros lo, cono,te declus médico, pero con estaba endla destomerdo en la cambio, cono,te cualmente, el desde con tento, el segudo, y una mucha exe comento, son el médicla, lomblamente rales cuallle lenta a conta, a los mueclo la ce contrar un perd\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 6500, Pérdida: 34.0433\n",
      "Generado: a la grervo. ya me la gregorio, pero la quer, porque tado de conitado de se hable, la pero a serlo de madre lo mago cír darse sasí. padre una quenas mera de pitó a la cambio. querro, se lis profundas circun poro padría quera madremente mozó de probre la voz de las circuncia de querados pata se grego\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 7000, Pérdida: 32.8178\n",
      "Generado: este meba al ciasustó habían, para de gorio,tera la puerta latengo deja encuentró la deja la levente encontraba bien. gregorio, pero to, gregorio, gregorio, gregorio, encuentaba no istar, era las lartado más gregorio, las si la puerta entraba enforme gia las se la la la gregorio, en voz nsusas las m\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 7500, Pérdida: 32.2664\n",
      "Generado: ión lo hado a suo mario despercespietera de cabe con inspuesa, precancionida sus las puertición lo hubión de cuerco pero las sus padre llitar surrtió hación el la menor intención de canceción las puertitó. on la menor la segiar des. pero la suplido durarquir a sustrado de cuilió sus yuición el pudir\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 8000, Pérdida: 32.6101\n",
      "Generado: ave, y levantarse puetido, se estar maríar algin cama entre de si lo cure con vesibras cinco qunazós resuler desvanidado de se sibasr por denviddió, tenía caja cinción con suación, se se abies ocidjanecióno prirdas pués se le vez, ese de descurio, y lo volvió harcese enclo, poro con levantarse piónr\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 8500, Pérdida: 33.0989\n",
      "Generado: osione, pero en la cambio sabería si en la mancipio se en la vanticamás, en la ayanmas había a necoma de los miendo la evansiter, chusitido los mucha. que hallamento y que,s era sin se especialmente, para sin se especión, pero en su lugantes partiar en el mádrá y el era muchas pataluetenía cambio el\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 9000, Pérdida: 32.2364\n",
      "Generado: d emás podía ituras que de lo que por cierto como emás cinción. el promo, no inferior, que, porman lo que quería tería y que y de como de con esta parte pero estarmue tiento podía imanas y que porque no podíaba con conoía con la partoma inferior de su vicio, era con esta partos que sí, si no primer \n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "Iteración 9500, Pérdida: 31.3418\n",
      "Generado:  el rolor la cabeza caestra promecisamentementon lugar sacuerpo se pospués e cuelo ser infera sintid. las cama lugar del cuerpo produlor. señor otisacecama la cabeza cama la cabezar en el producidaba del rolia el más sablegar con la cama l qunchado que, preciónó el cosio li cama la cabeza, ciertó la\n",
      "--------------------------------------------------\n",
      "¡Checkpoint guardado! (Params + Adam states) en: checkpoint_kafka.npz\n",
      "¡Éxito! Parámetros guardados en: checkpoint_kafka.npz\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar diccionarios (del paso anterior que hicimos)\n",
    "with open(\"metamorfosis_limpio.txt\", 'r', encoding='utf-8') as f:\n",
    "    texto = f.read()\n",
    "chars = sorted(list(set(texto)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "# 2. Entrenar\n",
    "parameters = model(texto, char_to_ix, ix_to_char, num_iterations=10000, n_a=100, vocab_size=vocab_size)\n",
    "save_parameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ec06df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Pesos cargados correctamente! Tu red ya sabe escribir.\n",
      "--- Generación Top-K (Más variada) ---\n",
      "rese pues, en el angro esta posicabeza noger la cabeza, no peser le prinar en esta, el cuerpo de herida, era no perdento. perdentión lentar en esta, perdente pero recalmente, con lentrted de herida, pero la cabeza, no podía de prido dejando de prindo an de su erpeser en la cabeza, sación, se fuerza \n"
     ]
    }
   ],
   "source": [
    "# Cargar tus pesos entrenados\n",
    "mis_pesos = load_parameters('checkpoint_kafka.npz')\n",
    "\n",
    "# --- PRUEBA EL NUEVO SAMPLING ---\n",
    "print(\"--- Generación Top-K (Más variada) ---\")\n",
    "idx = sample_top_k(mis_pesos, char_to_ix, seed='w', temperature=1, k=3)\n",
    "print(''.join([ix_to_char[i] for i in idx]).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4831f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
